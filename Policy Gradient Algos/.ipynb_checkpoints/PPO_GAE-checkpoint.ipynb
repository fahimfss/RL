{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "superior-tomorrow",
   "metadata": {},
   "source": [
    "## PPO with GAE Implementation:\n",
    "\n",
    "Here I will solve the Reacher 20, novis environment (from udacity second project). The reacher environment can be found [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip).\n",
    "\n",
    "Source: [https://github.com/higgsfield/RL-Adventure-2/blob/master/2.gae.ipynb](https://github.com/higgsfield/RL-Adventure-2/blob/master/2.gae.ipynb)\n",
    "\n",
    "PPO with GAE basic pseudocode: \n",
    "- The agent takes NUM_STEPS in the environment, collecting states, actions, rewards, state values, log probability of actions (log of policy) and terminal states\n",
    "- Using the collected values, the agent computes GAE values\n",
    "- Then for PPO_EPOCHS times, the agent collects a minibatch, computes ratio, and calculates 2 surrogate values. (ratio: action_new_prob/action_old_prob, surr1: ratio * advantage, surr2: clamped ratio * advantage)\n",
    "- actor loss is calculated using the negative of min of the 2 surrogate values\n",
    "- critic loss is calculated using: (gae_return - state_value)^2\n",
    "- losses are backpropagated to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-inflation",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automated-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-profile",
   "metadata": {},
   "source": [
    "### 2. Define Policy Network\n",
    "\n",
    "Very simple fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriented-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\" Initializes the weights and bias values of the Linear layers in the network \"\"\"\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "class A2CNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, std=0.0):\n",
    "        super(A2CNetwork, self).__init__()\n",
    "\n",
    "        # the critic network, it's output represents the state value for baseline\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # the actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, action_size),\n",
    "        )\n",
    "        \n",
    "        # value of log_std will also be updated by gradient update\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Returns distributions of actions and state value for input state \"\"\"\n",
    "\n",
    "        value = self.critic(x)\n",
    "        mu = torch.tanh(self.actor(x))\n",
    "\n",
    "        # calculating the standard deviation for the normal distribution\n",
    "        # if self.log_std is zeros tensor, then std is ones tensor\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "\n",
    "        # creating a normal distribution using mu and std\n",
    "        # using this normal distribution (dist), the action value will be sampled\n",
    "        dist = Normal(mu, std)\n",
    "\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-handbook",
   "metadata": {},
   "source": [
    "### 3. Define Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mobile-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper-parameters:\n",
    "NUM_STEPS = 20        # number of steps in rollout, increased in PPO\n",
    "LEARNING_RATE = 1e-5  # learning rate for the optimizer\n",
    "OPTIMIZER_EPS = 1e-3  # default eps value for optimizer might be too low for converging\n",
    "MINI_BATCH_SIZE  = 5\n",
    "PPO_EPOCHS       = 4\n",
    "\n",
    "\n",
    "class ReacherAgent:\n",
    "    \"\"\" The ReacherAgent class is responsible for interacting with the reacher environment and\n",
    "        teaching the neural network (model) to take appropriate actions based on states\n",
    "\n",
    "        This class is based on codes found here: \n",
    "        https://github.com/higgsfield/RL-Adventure-2/blob/master/2.gae.ipynb\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, env, brain_name, num_agents):\n",
    "        \"\"\"Initialize an ReacherAgent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            num_inputs (int): dimension of each state\n",
    "            num_outputs (int): dimension of each action\n",
    "            env: the reacher environment object\n",
    "            brain_name: reacher environment brain name\n",
    "            num_agents: number of agents in reacher environment\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = A2CNetwork(num_inputs, num_outputs).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE, eps=OPTIMIZER_EPS)\n",
    "        self.env = env\n",
    "        self.brain = brain_name\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # Initial state is stored in self.state. The states are normalized by dividing by 10.0\n",
    "        env_info = self.env.reset(train_mode=True)[self.brain]\n",
    "        self.state = env_info.vector_observations / 10.0\n",
    "\n",
    "        self.scores = np.zeros(self.num_agents)  # scores keeps track of the env rewards\n",
    "\n",
    "    def compute_gae(self, next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "        \"\"\" Returns the GAE (Generalized advantage estimation) value for a rollout, check \n",
    "        https://raw.githubusercontent.com/fahimfss/ProjectReacher/master/images_videos/compute_gae.png \n",
    "        for understanding how this method works\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            next_value: state value of the the last state in rollout\n",
    "            rewards: reward values during rollout\n",
    "            masks: terminal state masks of each state in rollout (1 means that state is terminal)\n",
    "            values: state values of each state in rollout\n",
    "            gamma: discount rate\n",
    "            tau: exponentially decaying factor for calculating GAE\n",
    "        \"\"\"\n",
    "\n",
    "        values = values + [next_value]\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + gamma * tau * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "    \n",
    "    def ppo_iter(self, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "        batch_size = states.size(0)\n",
    "        ids = np.random.permutation(batch_size)\n",
    "        ids = np.split(ids, batch_size // mini_batch_size)\n",
    "        for i in range(len(ids)):\n",
    "            yield states[ids[i], :], actions[ids[i], :], \\\n",
    "            log_probs[ids[i], :], returns[ids[i], :], advantage[ids[i], :]\n",
    "        \n",
    "        \n",
    "    def ppo_update(self, ppo_epochs, mini_batch_size, states, actions, \\\n",
    "                   log_probs, returns, advantages, clip_param=0.05):\n",
    "        for _ in range(ppo_epochs):\n",
    "            for state, action, old_log_probs, return_, advantage in \\\n",
    "            self.ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "                dist, value = self.model(state)\n",
    "                entropy = dist.entropy().mean()\n",
    "                new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                ratio = (new_log_probs - old_log_probs).exp()\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "                actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "                critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "                loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" step method does a NUM_STEPS rollout in the environment and updates the neural network (model)\n",
    "\n",
    "            :returns\n",
    "            The episode number and the mean score of agents in that episode if an episode ends in rollout\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs = []          # array to store log probabilities of each action in rollout\n",
    "        values    = []          # array to store state values (from model) of each state in rollout\n",
    "        rewards   = []          # array to store reward values for each step in rollout (provided by env)\n",
    "        states    = []\n",
    "        actions   = []\n",
    "        masks     = []          # array to store terminal state flags for each state in rollout\n",
    "\n",
    "        entropy = 0             # entropy term to encourage exploration\n",
    "\n",
    "        ret = None\n",
    "\n",
    "        for _ in range(NUM_STEPS):          # NUM_STEPS rollout\n",
    "\n",
    "            # Select action using the current state\n",
    "            state = torch.FloatTensor(self.state).to(device)\n",
    "            # the model returns a normal distribution for action value, and V(s)\n",
    "            dist, value = self.model(state)     \n",
    "            # action is sampled from the distribution and clipped in range [-1, 1]\n",
    "            action = dist.sample().clamp(-1, 1)   \n",
    "            action_np = action.cpu().numpy()\n",
    "\n",
    "            env_info = self.env.step(action_np)[self.brain]      # action is applied in the env\n",
    "            next_state = env_info.vector_observations / 10.0     # next_state is normalized by dividing by 10.0\n",
    "            reward = np.asarray(env_info.rewards) * 20.0         # rewards are amplified by multiplying by 20\n",
    "            done = np.array(env_info.local_done, dtype=int)      # done (terminal) flags are created\n",
    "\n",
    "            self.scores += env_info.rewards      # actual reward values are added to score\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            # related values are added to the arrays\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "            masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "            self.state = next_state\n",
    "\n",
    "            if np.any(done):    # for the given env, all the 20 environments terminate on the same frame\n",
    "                env_info = self.env.reset(train_mode=True)[self.brain]    # reset the env\n",
    "                self.state = env_info.vector_observations\n",
    "                # prepare return value, which is the mean of actual scores\n",
    "                ret = self.scores.mean()                      \n",
    "                self.scores = np.zeros(self.num_agents)       # reset scores\n",
    "\n",
    "        # state value of the last state is calculated using the neural network (model)\n",
    "\n",
    "        # print(\"entropy\", entropy)\n",
    "        # print(\"std\", self.model.log_std)\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        _, next_value = self.model(next_state)\n",
    "\n",
    "        # using the prepared arrays, the return values are calculated\n",
    "        returns = self.compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "        # convert arrays to torch tensors\n",
    "        log_probs = torch.cat(log_probs).detach()\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "        \n",
    "        self.ppo_update(PPO_EPOCHS, MINI_BATCH_SIZE, states, actions, log_probs, returns, advantage)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def act(self, state):\n",
    "        # given a state, returns the corresponding action\n",
    "\n",
    "        state = state / 10.0\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = self.model(state)  # the model returns a normal distribution for action value, and V(s)\n",
    "        # action is sampled from the distribution and clipped in range [-1, 1]\n",
    "        action = dist.sample().clamp(-1, 1)  \n",
    "        action_np = action.cpu().numpy()\n",
    "        return action_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-vancouver",
   "metadata": {},
   "source": [
    "### 4. Environment Sovler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-hearts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "Episode 100\tAverage Score: 25.4039644322\n",
      "Episode 121\tAverage Score: 32.0101692845"
     ]
    }
   ],
   "source": [
    "ENV_PATH = \"/home/fahim/Downloads/Reacher20_novis/Reacher.x86_64\"   # path to the reacher20 env\n",
    "SCORE_LIMIT = 32                              # mean score of 100 episodes to reach\n",
    "\n",
    "# initialize the environment\n",
    "env = UnityEnvironment(file_name=ENV_PATH)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "num_outputs = brain.vector_action_space_size\n",
    "print('Size of each action:', num_outputs)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "num_inputs = states.shape[1]\n",
    "\n",
    "scores = []  # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "# create the ReacherAgent object\n",
    "agent = ReacherAgent(num_inputs, num_outputs, env, brain_name, num_agents)\n",
    "i_episode = 0\n",
    "\n",
    "while True:  # train until the SCORE_LIMIT is not reached\n",
    "    reward = agent.step()\n",
    "    if reward is not None:\n",
    "        scores.append(reward)\n",
    "        scores_window.append(reward)\n",
    "        score_mean_100 = np.mean(scores_window)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.10f}'.format(i_episode, score_mean_100), end=\"\")\n",
    "\n",
    "        if i_episode > 0 and i_episode % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        # finish training once score limit is reached\n",
    "        if score_mean_100 > SCORE_LIMIT:\n",
    "            break\n",
    "\n",
    "        i_episode += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-halifax",
   "metadata": {},
   "source": [
    "### 5. Rewards plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "japanese-scenario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
