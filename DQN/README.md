# Meteors and DQN

### Overview
In this project, I created an RL agent that is able to play the game Meteors. I used Deep Q-Learning in this project. 
I experimented with various improvements of DQN and tested how those improvements worked out in playing Meteors. 

## Meteors
Meteors is a simple game made by me using [arcade](https://arcade.academy/). 
In the game, the player controls a rocket-ship and meteors (here unlimitedly and randomly generated by the game) come flying at the direction of the player's rocket. 
The player must avoid the meteors or the game ends. The player's score is increased by 1 when a meteor goes out of the screen.
   
![](https://github.com/fahimfss/RL/blob/master/DQN/MeteorGame/gifs_vids/run_human.gif?raw=true)

GIF: A human player playing Meteors. 
A more accurate representation (mp4) of the game can be found [here](https://github.com/fahimfss/RL/blob/master/DQN/MeteorGame/gifs_vids/run_human.mp4).

### Why Create A Game And Not Use OpenAI GYM environments
Mainly for these reasons: 
* I wanted to learn how custom environments can be created for RL
* Most of the DQN based implementations out there already use OpenAI GYM environments. I wanted to see how those implementation works in a custom environment
* Lastly, an RL agent that can play Meteors is a good first step. But ultimately, I want to create an RL agent that is able to generate the game (i.e. throw the meteors) 
so that a human player can play against it.   

## Meteors Environment

### Rewards and Episodes
The agent gets (+1 * scale) reward for each meteor that goes out of the screen. If a meteor hits the rocket-ship, the agent gets (-100 * scale) reward. 
The value of scale is set as (1/20). 
  
An episode is started with the rocket-ship on the left part of the screen and with no meteors on screen. The episode is concluded when a meteor hits the rocket-ship.

### State
A frame is a processed snapshot of the game at a particular time. A state consists of 4 frames. In the meteors environment, 
the same action is performed on 3 consecutive frames (skipped). 

### Action
The agent can perform 9 actions. 8 of the actions relate to moving in 8 directions (i.e. Up, Up-Left, Left, Left-Down, Down ...). Last action is actually not moving at all.


## The Learning Algorithm




